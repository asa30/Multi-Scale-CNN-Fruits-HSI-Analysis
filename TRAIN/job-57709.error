[38;5;127mmpi[0m/[38;5;172mopenmpi[0m/[38;5;67m4.1.5[0m/[38;5;68mgcc-4.8.5[0m
 | -- [38;5;127mlibs[0m/[38;5;172mgcc[0m/[38;5;67msystem[0m
 |    * --> [0;32mOK[0m
 |
 [0;32mOK[0m
2024-04-10 06:28:02.576038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-10 06:28:06.889146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-10 06:28:07.296511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43487 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:81:00.0, compute capability: 8.6
Traceback (most recent call last):
  File "/users/asa30/4thYearImplementation/TRAIN/avocado_vis_mkdcnna.py", line 37, in <module>
    model.fit(train_avocado_VIS_datagen, epochs=100)
  File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/tensorflow/python/eager/execute.py", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node 'model/depthwise_conv2d/depthwise' defined at (most recent call last):
    File "/users/asa30/4thYearImplementation/TRAIN/avocado_vis_mkdcnna.py", line 37, in <module>
      model.fit(train_avocado_VIS_datagen, epochs=100)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 1249, in train_function
      return step_function(self, iterator)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 1222, in run_step
      outputs = model.train_step(data)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 1023, in train_step
      y_pred = self(x, training=True)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/training.py", line 561, in __call__
      return super().__call__(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/base_layer.py", line 1132, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 96, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/functional.py", line 511, in call
      return self._run_internal_graph(inputs, training=training, mask=mask)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/functional.py", line 668, in _run_internal_graph
      outputs = node.layer(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/engine/base_layer.py", line 1132, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/utils/traceback_utils.py", line 96, in error_handler
      return fn(*args, **kwargs)
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/layers/convolutional/depthwise_conv2d.py", line 161, in call
      outputs = backend.depthwise_conv2d(
    File "/users/asa30/.local/share/flight/env/conda+asa30c/envs/tf-dmog/lib/python3.9/site-packages/keras/backend.py", line 6318, in depthwise_conv2d
      x = tf.compat.v1.nn.depthwise_conv2d(
Node: 'model/depthwise_conv2d/depthwise'
input and filter must have the same depth: 224 vs 252
	 [[{{node model/depthwise_conv2d/depthwise}}]] [Op:__inference_train_function_2210]
slurmstepd: error: *** JOB 57709 ON gpu06 CANCELLED AT 2024-04-11T08:17:08 ***
